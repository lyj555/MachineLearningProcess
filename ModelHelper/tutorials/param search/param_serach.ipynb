{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:17:48.371817Z",
     "start_time": "2019-12-03T09:17:48.284588Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from model_helper.hyper_parameter_tuning import param_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:17:51.254825Z",
     "start_time": "2019-12-03T09:17:50.781588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (5000, 300)\n",
      "label:  [1 1 0 ... 1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>f58</th>\n",
       "      <th>f59</th>\n",
       "      <th>f60</th>\n",
       "      <th>f61</th>\n",
       "      <th>f62</th>\n",
       "      <th>f63</th>\n",
       "      <th>f64</th>\n",
       "      <th>f65</th>\n",
       "      <th>f66</th>\n",
       "      <th>f67</th>\n",
       "      <th>f68</th>\n",
       "      <th>f69</th>\n",
       "      <th>f70</th>\n",
       "      <th>f71</th>\n",
       "      <th>...</th>\n",
       "      <th>f228</th>\n",
       "      <th>f229</th>\n",
       "      <th>f230</th>\n",
       "      <th>f231</th>\n",
       "      <th>f232</th>\n",
       "      <th>f233</th>\n",
       "      <th>f234</th>\n",
       "      <th>f235</th>\n",
       "      <th>f236</th>\n",
       "      <th>f237</th>\n",
       "      <th>f238</th>\n",
       "      <th>f239</th>\n",
       "      <th>f240</th>\n",
       "      <th>f241</th>\n",
       "      <th>f242</th>\n",
       "      <th>f243</th>\n",
       "      <th>f244</th>\n",
       "      <th>f245</th>\n",
       "      <th>f246</th>\n",
       "      <th>f247</th>\n",
       "      <th>f248</th>\n",
       "      <th>f249</th>\n",
       "      <th>f250</th>\n",
       "      <th>f251</th>\n",
       "      <th>f252</th>\n",
       "      <th>f253</th>\n",
       "      <th>f254</th>\n",
       "      <th>f255</th>\n",
       "      <th>f256</th>\n",
       "      <th>f257</th>\n",
       "      <th>f258</th>\n",
       "      <th>f259</th>\n",
       "      <th>f260</th>\n",
       "      <th>f261</th>\n",
       "      <th>f262</th>\n",
       "      <th>f263</th>\n",
       "      <th>f264</th>\n",
       "      <th>f265</th>\n",
       "      <th>f266</th>\n",
       "      <th>f267</th>\n",
       "      <th>f268</th>\n",
       "      <th>f269</th>\n",
       "      <th>f270</th>\n",
       "      <th>f271</th>\n",
       "      <th>f272</th>\n",
       "      <th>f273</th>\n",
       "      <th>f274</th>\n",
       "      <th>f275</th>\n",
       "      <th>f276</th>\n",
       "      <th>f277</th>\n",
       "      <th>f278</th>\n",
       "      <th>f279</th>\n",
       "      <th>f280</th>\n",
       "      <th>f281</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>f285</th>\n",
       "      <th>f286</th>\n",
       "      <th>f287</th>\n",
       "      <th>f288</th>\n",
       "      <th>f289</th>\n",
       "      <th>f290</th>\n",
       "      <th>f291</th>\n",
       "      <th>f292</th>\n",
       "      <th>f293</th>\n",
       "      <th>f294</th>\n",
       "      <th>f295</th>\n",
       "      <th>f296</th>\n",
       "      <th>f297</th>\n",
       "      <th>f298</th>\n",
       "      <th>f299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994557</td>\n",
       "      <td>-1.136878</td>\n",
       "      <td>0.169768</td>\n",
       "      <td>0.768031</td>\n",
       "      <td>0.014296</td>\n",
       "      <td>0.524148</td>\n",
       "      <td>1.023236</td>\n",
       "      <td>-0.172799</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>-0.310725</td>\n",
       "      <td>-0.360682</td>\n",
       "      <td>-0.009369</td>\n",
       "      <td>-1.745508</td>\n",
       "      <td>-0.285820</td>\n",
       "      <td>-0.732836</td>\n",
       "      <td>-8.454865</td>\n",
       "      <td>-0.552820</td>\n",
       "      <td>1.515700</td>\n",
       "      <td>0.150757</td>\n",
       "      <td>0.794646</td>\n",
       "      <td>-0.170764</td>\n",
       "      <td>0.158164</td>\n",
       "      <td>-1.026552</td>\n",
       "      <td>1.363109</td>\n",
       "      <td>-0.199189</td>\n",
       "      <td>-1.350810</td>\n",
       "      <td>-2.830320</td>\n",
       "      <td>-0.934891</td>\n",
       "      <td>0.282713</td>\n",
       "      <td>3.924545</td>\n",
       "      <td>-0.440672</td>\n",
       "      <td>0.833457</td>\n",
       "      <td>1.067953</td>\n",
       "      <td>1.716262</td>\n",
       "      <td>-0.412540</td>\n",
       "      <td>-1.456003</td>\n",
       "      <td>-0.857324</td>\n",
       "      <td>-0.537075</td>\n",
       "      <td>0.504901</td>\n",
       "      <td>1.137019</td>\n",
       "      <td>-1.734692</td>\n",
       "      <td>0.088075</td>\n",
       "      <td>-0.096549</td>\n",
       "      <td>-0.432578</td>\n",
       "      <td>0.608646</td>\n",
       "      <td>-0.306824</td>\n",
       "      <td>-0.213374</td>\n",
       "      <td>-0.354016</td>\n",
       "      <td>0.367735</td>\n",
       "      <td>-0.342739</td>\n",
       "      <td>0.206341</td>\n",
       "      <td>-0.429546</td>\n",
       "      <td>-0.602545</td>\n",
       "      <td>-0.119167</td>\n",
       "      <td>0.326396</td>\n",
       "      <td>-0.538259</td>\n",
       "      <td>0.898635</td>\n",
       "      <td>-2.332112</td>\n",
       "      <td>2.669598</td>\n",
       "      <td>0.840676</td>\n",
       "      <td>-0.797064</td>\n",
       "      <td>-0.469316</td>\n",
       "      <td>0.563072</td>\n",
       "      <td>-1.437217</td>\n",
       "      <td>2.172028</td>\n",
       "      <td>0.300796</td>\n",
       "      <td>-0.205605</td>\n",
       "      <td>-0.226380</td>\n",
       "      <td>-0.496111</td>\n",
       "      <td>-0.789304</td>\n",
       "      <td>-0.538536</td>\n",
       "      <td>-0.029335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335241</td>\n",
       "      <td>-1.007284</td>\n",
       "      <td>1.640652</td>\n",
       "      <td>3.398284</td>\n",
       "      <td>-1.582772</td>\n",
       "      <td>0.293247</td>\n",
       "      <td>0.388747</td>\n",
       "      <td>-1.167973</td>\n",
       "      <td>0.441490</td>\n",
       "      <td>0.556139</td>\n",
       "      <td>-0.545364</td>\n",
       "      <td>-1.025100</td>\n",
       "      <td>-0.927151</td>\n",
       "      <td>-0.425784</td>\n",
       "      <td>0.360076</td>\n",
       "      <td>0.805664</td>\n",
       "      <td>-0.549137</td>\n",
       "      <td>-0.537014</td>\n",
       "      <td>-0.188337</td>\n",
       "      <td>0.024976</td>\n",
       "      <td>-0.721833</td>\n",
       "      <td>0.141061</td>\n",
       "      <td>-0.121944</td>\n",
       "      <td>0.754324</td>\n",
       "      <td>0.332226</td>\n",
       "      <td>-1.185077</td>\n",
       "      <td>0.838498</td>\n",
       "      <td>-0.226968</td>\n",
       "      <td>-13.374565</td>\n",
       "      <td>-0.757486</td>\n",
       "      <td>0.461798</td>\n",
       "      <td>-1.089996</td>\n",
       "      <td>1.862257</td>\n",
       "      <td>0.417729</td>\n",
       "      <td>-0.297050</td>\n",
       "      <td>-0.482758</td>\n",
       "      <td>0.464344</td>\n",
       "      <td>2.146531</td>\n",
       "      <td>1.513262</td>\n",
       "      <td>-1.389643</td>\n",
       "      <td>-0.473430</td>\n",
       "      <td>-0.597523</td>\n",
       "      <td>0.233291</td>\n",
       "      <td>0.774206</td>\n",
       "      <td>0.689711</td>\n",
       "      <td>-0.382393</td>\n",
       "      <td>-1.162316</td>\n",
       "      <td>-1.234355</td>\n",
       "      <td>2.416240</td>\n",
       "      <td>-2.876688</td>\n",
       "      <td>-0.041498</td>\n",
       "      <td>0.539115</td>\n",
       "      <td>-0.049585</td>\n",
       "      <td>-1.331033</td>\n",
       "      <td>-0.717556</td>\n",
       "      <td>-0.521484</td>\n",
       "      <td>0.139602</td>\n",
       "      <td>-0.199763</td>\n",
       "      <td>3.456952</td>\n",
       "      <td>-0.333085</td>\n",
       "      <td>-0.455004</td>\n",
       "      <td>-0.072169</td>\n",
       "      <td>1.074111</td>\n",
       "      <td>-0.392414</td>\n",
       "      <td>-0.380007</td>\n",
       "      <td>1.463322</td>\n",
       "      <td>0.950080</td>\n",
       "      <td>-0.374694</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>-0.036063</td>\n",
       "      <td>-0.910353</td>\n",
       "      <td>-0.137185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.601184</td>\n",
       "      <td>-0.470369</td>\n",
       "      <td>-1.054326</td>\n",
       "      <td>0.352207</td>\n",
       "      <td>-0.431754</td>\n",
       "      <td>-0.186422</td>\n",
       "      <td>1.362683</td>\n",
       "      <td>-0.126976</td>\n",
       "      <td>-2.522448</td>\n",
       "      <td>0.626738</td>\n",
       "      <td>0.509184</td>\n",
       "      <td>1.073414</td>\n",
       "      <td>0.970938</td>\n",
       "      <td>-1.163877</td>\n",
       "      <td>0.368396</td>\n",
       "      <td>-0.342664</td>\n",
       "      <td>1.372190</td>\n",
       "      <td>2.367848</td>\n",
       "      <td>-0.032828</td>\n",
       "      <td>0.296787</td>\n",
       "      <td>0.947888</td>\n",
       "      <td>-1.290605</td>\n",
       "      <td>-0.726592</td>\n",
       "      <td>-0.508369</td>\n",
       "      <td>-0.453301</td>\n",
       "      <td>-0.471973</td>\n",
       "      <td>0.262014</td>\n",
       "      <td>0.221347</td>\n",
       "      <td>0.843240</td>\n",
       "      <td>3.416713</td>\n",
       "      <td>0.908989</td>\n",
       "      <td>1.455648</td>\n",
       "      <td>-0.516664</td>\n",
       "      <td>0.435060</td>\n",
       "      <td>-0.705554</td>\n",
       "      <td>0.606660</td>\n",
       "      <td>-0.108290</td>\n",
       "      <td>0.594338</td>\n",
       "      <td>0.317117</td>\n",
       "      <td>-0.350836</td>\n",
       "      <td>-0.175509</td>\n",
       "      <td>-0.438238</td>\n",
       "      <td>-1.652411</td>\n",
       "      <td>0.350066</td>\n",
       "      <td>1.259202</td>\n",
       "      <td>2.180739</td>\n",
       "      <td>1.574884</td>\n",
       "      <td>-0.542642</td>\n",
       "      <td>0.565118</td>\n",
       "      <td>1.645375</td>\n",
       "      <td>0.376335</td>\n",
       "      <td>-0.809269</td>\n",
       "      <td>2.130345</td>\n",
       "      <td>-0.122436</td>\n",
       "      <td>0.291321</td>\n",
       "      <td>-2.206792</td>\n",
       "      <td>1.080359</td>\n",
       "      <td>-0.768021</td>\n",
       "      <td>1.541170</td>\n",
       "      <td>1.185313</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.018902</td>\n",
       "      <td>1.335862</td>\n",
       "      <td>0.730105</td>\n",
       "      <td>-0.836299</td>\n",
       "      <td>0.848551</td>\n",
       "      <td>1.466890</td>\n",
       "      <td>-0.021652</td>\n",
       "      <td>0.897386</td>\n",
       "      <td>-0.291702</td>\n",
       "      <td>-1.820991</td>\n",
       "      <td>-0.250441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240545</td>\n",
       "      <td>0.505837</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.273111</td>\n",
       "      <td>1.070617</td>\n",
       "      <td>-0.859533</td>\n",
       "      <td>-0.231231</td>\n",
       "      <td>2.357933</td>\n",
       "      <td>-0.431621</td>\n",
       "      <td>0.130922</td>\n",
       "      <td>-0.343335</td>\n",
       "      <td>1.509881</td>\n",
       "      <td>-1.085815</td>\n",
       "      <td>1.128897</td>\n",
       "      <td>-0.882096</td>\n",
       "      <td>-0.621342</td>\n",
       "      <td>-0.920800</td>\n",
       "      <td>0.460677</td>\n",
       "      <td>0.505478</td>\n",
       "      <td>1.326087</td>\n",
       "      <td>-2.296611</td>\n",
       "      <td>-1.283698</td>\n",
       "      <td>0.265664</td>\n",
       "      <td>0.142616</td>\n",
       "      <td>-1.747888</td>\n",
       "      <td>0.045265</td>\n",
       "      <td>0.050653</td>\n",
       "      <td>0.778549</td>\n",
       "      <td>-0.570501</td>\n",
       "      <td>-0.099234</td>\n",
       "      <td>0.833948</td>\n",
       "      <td>0.856969</td>\n",
       "      <td>-1.458339</td>\n",
       "      <td>-0.655688</td>\n",
       "      <td>-0.980949</td>\n",
       "      <td>-0.194226</td>\n",
       "      <td>-1.953573</td>\n",
       "      <td>-0.193864</td>\n",
       "      <td>-0.552708</td>\n",
       "      <td>1.055110</td>\n",
       "      <td>-0.972686</td>\n",
       "      <td>-0.216315</td>\n",
       "      <td>0.988420</td>\n",
       "      <td>0.107106</td>\n",
       "      <td>-0.412148</td>\n",
       "      <td>0.600358</td>\n",
       "      <td>0.311073</td>\n",
       "      <td>-0.838539</td>\n",
       "      <td>0.846744</td>\n",
       "      <td>-0.952445</td>\n",
       "      <td>-0.649137</td>\n",
       "      <td>-1.422914</td>\n",
       "      <td>-0.915237</td>\n",
       "      <td>0.898117</td>\n",
       "      <td>-0.650475</td>\n",
       "      <td>-0.526929</td>\n",
       "      <td>-0.260181</td>\n",
       "      <td>1.378182</td>\n",
       "      <td>1.964127</td>\n",
       "      <td>-0.401667</td>\n",
       "      <td>1.823332</td>\n",
       "      <td>-0.984320</td>\n",
       "      <td>0.346330</td>\n",
       "      <td>-2.165880</td>\n",
       "      <td>2.037716</td>\n",
       "      <td>-0.715749</td>\n",
       "      <td>-0.328964</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>-1.090315</td>\n",
       "      <td>-0.632209</td>\n",
       "      <td>0.886497</td>\n",
       "      <td>0.914958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.597741</td>\n",
       "      <td>0.777435</td>\n",
       "      <td>0.267835</td>\n",
       "      <td>-1.186580</td>\n",
       "      <td>-0.048285</td>\n",
       "      <td>0.638156</td>\n",
       "      <td>0.627173</td>\n",
       "      <td>-0.367807</td>\n",
       "      <td>-2.083738</td>\n",
       "      <td>1.277261</td>\n",
       "      <td>0.233863</td>\n",
       "      <td>0.131141</td>\n",
       "      <td>-1.373667</td>\n",
       "      <td>0.414823</td>\n",
       "      <td>-0.173497</td>\n",
       "      <td>5.956362</td>\n",
       "      <td>-0.149717</td>\n",
       "      <td>1.260773</td>\n",
       "      <td>-1.064643</td>\n",
       "      <td>-1.312648</td>\n",
       "      <td>1.122190</td>\n",
       "      <td>-0.171009</td>\n",
       "      <td>0.267673</td>\n",
       "      <td>-1.726734</td>\n",
       "      <td>0.119917</td>\n",
       "      <td>-1.501998</td>\n",
       "      <td>-1.165411</td>\n",
       "      <td>-0.932710</td>\n",
       "      <td>1.319535</td>\n",
       "      <td>-1.861041</td>\n",
       "      <td>-0.733492</td>\n",
       "      <td>-0.234975</td>\n",
       "      <td>0.069513</td>\n",
       "      <td>0.381036</td>\n",
       "      <td>1.050863</td>\n",
       "      <td>-0.148127</td>\n",
       "      <td>-0.577237</td>\n",
       "      <td>-2.538462</td>\n",
       "      <td>-1.738688</td>\n",
       "      <td>0.129584</td>\n",
       "      <td>-0.485468</td>\n",
       "      <td>1.630482</td>\n",
       "      <td>-0.478737</td>\n",
       "      <td>0.139484</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>-0.633077</td>\n",
       "      <td>-0.062994</td>\n",
       "      <td>-0.693916</td>\n",
       "      <td>-0.151163</td>\n",
       "      <td>0.747786</td>\n",
       "      <td>0.378337</td>\n",
       "      <td>-0.456834</td>\n",
       "      <td>-0.633313</td>\n",
       "      <td>0.411016</td>\n",
       "      <td>-0.996164</td>\n",
       "      <td>1.967859</td>\n",
       "      <td>1.168394</td>\n",
       "      <td>-0.959444</td>\n",
       "      <td>-0.832629</td>\n",
       "      <td>-0.326027</td>\n",
       "      <td>1.202331</td>\n",
       "      <td>0.348863</td>\n",
       "      <td>-0.542074</td>\n",
       "      <td>0.568868</td>\n",
       "      <td>-1.227099</td>\n",
       "      <td>1.106278</td>\n",
       "      <td>0.461695</td>\n",
       "      <td>0.985369</td>\n",
       "      <td>0.756630</td>\n",
       "      <td>-0.234230</td>\n",
       "      <td>-0.714289</td>\n",
       "      <td>-0.067891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>0.464393</td>\n",
       "      <td>-0.876301</td>\n",
       "      <td>4.926311</td>\n",
       "      <td>-1.258687</td>\n",
       "      <td>-1.438384</td>\n",
       "      <td>-0.070598</td>\n",
       "      <td>-0.645337</td>\n",
       "      <td>-1.486247</td>\n",
       "      <td>-1.592182</td>\n",
       "      <td>1.736985</td>\n",
       "      <td>1.889882</td>\n",
       "      <td>0.732795</td>\n",
       "      <td>-0.173453</td>\n",
       "      <td>-0.675912</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.897642</td>\n",
       "      <td>-2.905841</td>\n",
       "      <td>-0.129962</td>\n",
       "      <td>-2.894047</td>\n",
       "      <td>0.517944</td>\n",
       "      <td>-0.794494</td>\n",
       "      <td>1.999267</td>\n",
       "      <td>-1.143539</td>\n",
       "      <td>1.226223</td>\n",
       "      <td>3.712762</td>\n",
       "      <td>-0.088807</td>\n",
       "      <td>1.627577</td>\n",
       "      <td>3.178682</td>\n",
       "      <td>-0.068179</td>\n",
       "      <td>0.654648</td>\n",
       "      <td>1.207134</td>\n",
       "      <td>-0.142716</td>\n",
       "      <td>-0.357668</td>\n",
       "      <td>-0.002619</td>\n",
       "      <td>-0.692077</td>\n",
       "      <td>1.537404</td>\n",
       "      <td>1.095600</td>\n",
       "      <td>-1.141594</td>\n",
       "      <td>-0.053931</td>\n",
       "      <td>-0.119170</td>\n",
       "      <td>-0.141481</td>\n",
       "      <td>0.497925</td>\n",
       "      <td>0.284489</td>\n",
       "      <td>0.950702</td>\n",
       "      <td>0.765780</td>\n",
       "      <td>-1.023197</td>\n",
       "      <td>-2.161261</td>\n",
       "      <td>-0.355705</td>\n",
       "      <td>-0.546029</td>\n",
       "      <td>-0.213186</td>\n",
       "      <td>-0.491287</td>\n",
       "      <td>0.303025</td>\n",
       "      <td>-0.850505</td>\n",
       "      <td>0.982653</td>\n",
       "      <td>0.698245</td>\n",
       "      <td>0.820335</td>\n",
       "      <td>-0.367143</td>\n",
       "      <td>-1.703033</td>\n",
       "      <td>-0.180803</td>\n",
       "      <td>-0.101001</td>\n",
       "      <td>0.143985</td>\n",
       "      <td>-2.057728</td>\n",
       "      <td>1.391847</td>\n",
       "      <td>-0.760141</td>\n",
       "      <td>-0.700443</td>\n",
       "      <td>-3.046240</td>\n",
       "      <td>0.323482</td>\n",
       "      <td>1.074514</td>\n",
       "      <td>-1.064665</td>\n",
       "      <td>-1.457375</td>\n",
       "      <td>0.504237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.909784</td>\n",
       "      <td>0.910874</td>\n",
       "      <td>0.435565</td>\n",
       "      <td>-1.951355</td>\n",
       "      <td>0.577979</td>\n",
       "      <td>-0.603668</td>\n",
       "      <td>-0.209334</td>\n",
       "      <td>0.787032</td>\n",
       "      <td>-2.518156</td>\n",
       "      <td>-0.189924</td>\n",
       "      <td>-1.456707</td>\n",
       "      <td>-0.332381</td>\n",
       "      <td>-1.351855</td>\n",
       "      <td>-1.515931</td>\n",
       "      <td>0.918289</td>\n",
       "      <td>4.017587</td>\n",
       "      <td>-0.956495</td>\n",
       "      <td>-0.621731</td>\n",
       "      <td>0.255666</td>\n",
       "      <td>0.711909</td>\n",
       "      <td>-1.155692</td>\n",
       "      <td>0.230411</td>\n",
       "      <td>-0.942624</td>\n",
       "      <td>-1.318674</td>\n",
       "      <td>0.746506</td>\n",
       "      <td>0.800020</td>\n",
       "      <td>-2.354510</td>\n",
       "      <td>-0.717182</td>\n",
       "      <td>1.741811</td>\n",
       "      <td>-3.758324</td>\n",
       "      <td>1.339042</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>-0.585986</td>\n",
       "      <td>1.256353</td>\n",
       "      <td>1.179539</td>\n",
       "      <td>0.703261</td>\n",
       "      <td>0.724731</td>\n",
       "      <td>0.354745</td>\n",
       "      <td>-0.120537</td>\n",
       "      <td>0.349746</td>\n",
       "      <td>-1.570842</td>\n",
       "      <td>-0.910761</td>\n",
       "      <td>-2.001774</td>\n",
       "      <td>0.631649</td>\n",
       "      <td>-1.680781</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>-0.059625</td>\n",
       "      <td>0.203114</td>\n",
       "      <td>-0.500091</td>\n",
       "      <td>-0.025850</td>\n",
       "      <td>-0.315273</td>\n",
       "      <td>0.204220</td>\n",
       "      <td>1.095041</td>\n",
       "      <td>1.985156</td>\n",
       "      <td>0.432752</td>\n",
       "      <td>-1.013223</td>\n",
       "      <td>-0.092385</td>\n",
       "      <td>-0.157620</td>\n",
       "      <td>-0.166755</td>\n",
       "      <td>-0.585622</td>\n",
       "      <td>1.310983</td>\n",
       "      <td>-0.116419</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.839376</td>\n",
       "      <td>-0.927028</td>\n",
       "      <td>0.205657</td>\n",
       "      <td>0.674152</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>-0.825423</td>\n",
       "      <td>-0.948867</td>\n",
       "      <td>-0.049615</td>\n",
       "      <td>0.585919</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.021141</td>\n",
       "      <td>0.286586</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>-4.902816</td>\n",
       "      <td>0.015721</td>\n",
       "      <td>0.355722</td>\n",
       "      <td>-1.775694</td>\n",
       "      <td>0.204048</td>\n",
       "      <td>-0.590624</td>\n",
       "      <td>1.063819</td>\n",
       "      <td>0.505151</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>1.924744</td>\n",
       "      <td>0.186532</td>\n",
       "      <td>-0.530073</td>\n",
       "      <td>-0.442152</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>-0.529442</td>\n",
       "      <td>0.783219</td>\n",
       "      <td>-3.185634</td>\n",
       "      <td>-0.508324</td>\n",
       "      <td>-0.684590</td>\n",
       "      <td>0.124627</td>\n",
       "      <td>-0.982565</td>\n",
       "      <td>-0.278718</td>\n",
       "      <td>-4.208872</td>\n",
       "      <td>0.200652</td>\n",
       "      <td>-0.131117</td>\n",
       "      <td>4.688569</td>\n",
       "      <td>-0.242202</td>\n",
       "      <td>0.879977</td>\n",
       "      <td>-0.752611</td>\n",
       "      <td>-0.346746</td>\n",
       "      <td>-0.629414</td>\n",
       "      <td>1.621588</td>\n",
       "      <td>0.469005</td>\n",
       "      <td>0.039196</td>\n",
       "      <td>-0.303846</td>\n",
       "      <td>-0.006462</td>\n",
       "      <td>-0.416800</td>\n",
       "      <td>0.197389</td>\n",
       "      <td>1.114280</td>\n",
       "      <td>-1.510443</td>\n",
       "      <td>0.355572</td>\n",
       "      <td>1.483230</td>\n",
       "      <td>-0.341241</td>\n",
       "      <td>-0.293479</td>\n",
       "      <td>-1.548713</td>\n",
       "      <td>-0.108598</td>\n",
       "      <td>-0.179256</td>\n",
       "      <td>0.726325</td>\n",
       "      <td>1.829506</td>\n",
       "      <td>-1.581339</td>\n",
       "      <td>0.044962</td>\n",
       "      <td>0.442572</td>\n",
       "      <td>2.442752</td>\n",
       "      <td>-1.817412</td>\n",
       "      <td>0.798624</td>\n",
       "      <td>-5.368131</td>\n",
       "      <td>1.044332</td>\n",
       "      <td>-1.960260</td>\n",
       "      <td>1.915259</td>\n",
       "      <td>0.052121</td>\n",
       "      <td>0.144603</td>\n",
       "      <td>0.474985</td>\n",
       "      <td>0.024548</td>\n",
       "      <td>0.138589</td>\n",
       "      <td>1.339473</td>\n",
       "      <td>0.518763</td>\n",
       "      <td>0.312907</td>\n",
       "      <td>0.992612</td>\n",
       "      <td>-1.121993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.121554</td>\n",
       "      <td>-1.045082</td>\n",
       "      <td>-1.070315</td>\n",
       "      <td>-0.441583</td>\n",
       "      <td>-1.718309</td>\n",
       "      <td>0.138502</td>\n",
       "      <td>-0.534873</td>\n",
       "      <td>2.212596</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>-1.008905</td>\n",
       "      <td>-1.052491</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.613178</td>\n",
       "      <td>0.604801</td>\n",
       "      <td>0.435008</td>\n",
       "      <td>0.331924</td>\n",
       "      <td>-0.420090</td>\n",
       "      <td>0.318561</td>\n",
       "      <td>0.099825</td>\n",
       "      <td>0.201441</td>\n",
       "      <td>0.607827</td>\n",
       "      <td>-1.110293</td>\n",
       "      <td>-0.754490</td>\n",
       "      <td>0.939156</td>\n",
       "      <td>2.509579</td>\n",
       "      <td>-0.933295</td>\n",
       "      <td>0.219973</td>\n",
       "      <td>-1.389583</td>\n",
       "      <td>-0.304271</td>\n",
       "      <td>3.290526</td>\n",
       "      <td>0.083557</td>\n",
       "      <td>-0.194253</td>\n",
       "      <td>1.990401</td>\n",
       "      <td>1.237638</td>\n",
       "      <td>0.521053</td>\n",
       "      <td>-0.056600</td>\n",
       "      <td>-0.304837</td>\n",
       "      <td>-0.008689</td>\n",
       "      <td>-1.317874</td>\n",
       "      <td>0.860380</td>\n",
       "      <td>-0.063057</td>\n",
       "      <td>2.087216</td>\n",
       "      <td>-0.714741</td>\n",
       "      <td>-1.198157</td>\n",
       "      <td>0.470778</td>\n",
       "      <td>0.929448</td>\n",
       "      <td>2.205578</td>\n",
       "      <td>-0.585321</td>\n",
       "      <td>-0.107177</td>\n",
       "      <td>-0.201145</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>-0.272854</td>\n",
       "      <td>-0.264822</td>\n",
       "      <td>0.950107</td>\n",
       "      <td>0.061240</td>\n",
       "      <td>0.671026</td>\n",
       "      <td>1.884223</td>\n",
       "      <td>0.211707</td>\n",
       "      <td>0.363608</td>\n",
       "      <td>-1.936927</td>\n",
       "      <td>0.301095</td>\n",
       "      <td>-0.402428</td>\n",
       "      <td>0.094371</td>\n",
       "      <td>0.859033</td>\n",
       "      <td>-1.949950</td>\n",
       "      <td>-0.591759</td>\n",
       "      <td>-1.194383</td>\n",
       "      <td>0.688613</td>\n",
       "      <td>0.413873</td>\n",
       "      <td>0.166518</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>...</td>\n",
       "      <td>2.315708</td>\n",
       "      <td>0.610742</td>\n",
       "      <td>2.062684</td>\n",
       "      <td>2.619779</td>\n",
       "      <td>1.601514</td>\n",
       "      <td>-0.842461</td>\n",
       "      <td>0.564719</td>\n",
       "      <td>0.777999</td>\n",
       "      <td>0.044064</td>\n",
       "      <td>0.112837</td>\n",
       "      <td>-0.049014</td>\n",
       "      <td>-0.797425</td>\n",
       "      <td>-0.820319</td>\n",
       "      <td>-1.656572</td>\n",
       "      <td>-0.555399</td>\n",
       "      <td>1.052226</td>\n",
       "      <td>-0.932612</td>\n",
       "      <td>0.423432</td>\n",
       "      <td>0.357920</td>\n",
       "      <td>-0.784020</td>\n",
       "      <td>-0.626823</td>\n",
       "      <td>0.629402</td>\n",
       "      <td>0.754399</td>\n",
       "      <td>-1.523319</td>\n",
       "      <td>-1.039588</td>\n",
       "      <td>0.692800</td>\n",
       "      <td>0.414251</td>\n",
       "      <td>0.783829</td>\n",
       "      <td>2.097237</td>\n",
       "      <td>1.640938</td>\n",
       "      <td>1.461065</td>\n",
       "      <td>-1.741251</td>\n",
       "      <td>-1.124331</td>\n",
       "      <td>-1.522300</td>\n",
       "      <td>1.321969</td>\n",
       "      <td>0.911481</td>\n",
       "      <td>0.615799</td>\n",
       "      <td>1.010708</td>\n",
       "      <td>0.470715</td>\n",
       "      <td>-0.527636</td>\n",
       "      <td>0.056790</td>\n",
       "      <td>1.147405</td>\n",
       "      <td>-0.875006</td>\n",
       "      <td>-0.907738</td>\n",
       "      <td>-1.183286</td>\n",
       "      <td>2.802049</td>\n",
       "      <td>0.492197</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>-0.223270</td>\n",
       "      <td>-2.317633</td>\n",
       "      <td>0.339907</td>\n",
       "      <td>-0.082266</td>\n",
       "      <td>-0.118919</td>\n",
       "      <td>0.788914</td>\n",
       "      <td>-0.317833</td>\n",
       "      <td>-1.461239</td>\n",
       "      <td>-0.382739</td>\n",
       "      <td>-2.467649</td>\n",
       "      <td>0.375166</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>-0.404333</td>\n",
       "      <td>1.267492</td>\n",
       "      <td>2.010374</td>\n",
       "      <td>-1.055319</td>\n",
       "      <td>0.852120</td>\n",
       "      <td>-2.115959</td>\n",
       "      <td>1.801926</td>\n",
       "      <td>-0.327890</td>\n",
       "      <td>-1.539215</td>\n",
       "      <td>0.214853</td>\n",
       "      <td>2.003376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4        f5    ...         f294      f295      f296      f297      f298      f299\n",
       "0 -0.994557 -1.136878  0.169768  0.768031  0.014296  0.524148    ...     0.950080 -0.374694  0.024485 -0.036063 -0.910353 -0.137185\n",
       "1 -0.601184 -0.470369 -1.054326  0.352207 -0.431754 -0.186422    ...    -0.328964  0.091847 -1.090315 -0.632209  0.886497  0.914958\n",
       "2 -1.597741  0.777435  0.267835 -1.186580 -0.048285  0.638156    ...    -3.046240  0.323482  1.074514 -1.064665 -1.457375  0.504237\n",
       "3  0.909784  0.910874  0.435565 -1.951355  0.577979 -0.603668    ...     0.138589  1.339473  0.518763  0.312907  0.992612 -1.121993\n",
       "4  0.121554 -1.045082 -1.070315 -0.441583 -1.718309  0.138502    ...    -2.115959  1.801926 -0.327890 -1.539215  0.214853  2.003376\n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建测试数据\n",
    "df, label = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=300,\n",
    "    n_informative=12,\n",
    "    n_redundant=7,\n",
    "    random_state=134985745,\n",
    ")\n",
    "df = pd.DataFrame(df, columns=[f\"f{i}\" for i in range(df.shape[1])])\n",
    "\n",
    "print(\"df shape:\", df.shape)\n",
    "print(\"label: \", label)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数选择\n",
    "\n",
    "目前超参数选择部分，共实现了四种算法，分别是网格搜索、随机搜索、bayes_opt和hyper_opt，下面详细介绍这几种方式的使用方法。\n",
    "\n",
    "其中网格搜索和随机搜索这两种方式放置于同一个方法中，首先介绍这两种方式，\n",
    "\n",
    "## 1. param_search\n",
    "\n",
    "网格搜索搜索，即是从超参数的网格中给出所有可能的参数组合，然后依次遍历每一个参数组合，看一下哪个参数组合效果最优；而随机搜索则是在网格搜索的简化版，在所有可能的组合中随机抽取部分参数组合进行遍历，然后选取最优，往往来说，如果所制定的参数组合较多且数据量较大时，可以考虑随机搜索，可以加快超参数选择的速度。\n",
    "### 1.1 方法1\n",
    "该方法使用交叉验证进行参数组合的评分，最终返回最优效果和最佳参数组合，下面看代码使用，\n",
    "\n",
    "> 内部交叉验证的实现是调用的sklearn的`cross_val_score`方法进行实现，相应的参数传递可以参考下面方法的介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:18:49.723519Z",
     "start_time": "2019-12-03T09:17:55.208088Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize effect 0.7664000213162648, cost time 5, with feat_dim 300, with param {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "round 1/32 start...\n",
      "round 1/32 end, effect subset is 0.7555990602599767, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 2/32 start...\n",
      "round 2/32 end, effect subset is 0.7789999023004524, cost time 2, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 3/32 start...\n",
      "round 3/32 end, effect subset is 0.7949986233245547, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 4/32 start...\n",
      "round 4/32 end, effect subset is 0.7567997805000825, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 5/32 start...\n",
      "round 5/32 end, effect subset is 0.7567997805000825, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "round 6/32 start...\n",
      "round 6/32 end, effect subset is 0.7804008225965852, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 7/32 start...\n",
      "round 7/32 end, effect subset is 0.7555990602599767, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 8/32 start...\n",
      "round 8/32 end, effect subset is 0.7567997805000825, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 9/32 start...\n",
      "round 9/32 end, effect subset is 0.7951984633085388, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 10/32 start...\n",
      "round 10/32 end, effect subset is 0.7561999004760872, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 11/32 start...\n",
      "round 11/32 end, effect subset is 0.7555990602599767, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 12/32 start...\n",
      "round 12/32 end, effect subset is 0.7623981806279799, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 13/32 start...\n",
      "round 13/32 end, effect subset is 0.7618004610362442, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 14/32 start...\n",
      "round 14/32 end, effect subset is 0.7555990602599767, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 15/32 start...\n",
      "round 15/32 end, effect subset is 0.7778001422524619, cost time 2, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 16/32 start...\n",
      "round 16/32 end, effect subset is 0.8033997042128189, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 17/32 start...\n",
      "round 17/32 end, effect subset is 0.7745971814040553, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 18/32 start...\n",
      "round 18/32 end, effect subset is 0.8061998644688829, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 19/32 start...\n",
      "round 19/32 end, effect subset is 0.7561999004760872, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "round 20/32 start...\n",
      "round 20/32 end, effect subset is 0.7603985805479961, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 21/32 start...\n",
      "round 21/32 end, effect subset is 0.7603985805479961, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 22/32 start...\n",
      "round 22/32 end, effect subset is 0.7843995426604954, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 23/32 start...\n",
      "round 23/32 end, effect subset is 0.7789999023004524, cost time 2, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 24/32 start...\n",
      "round 24/32 end, effect subset is 0.7561999004760872, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 25/32 start...\n",
      "round 25/32 end, effect subset is 0.7567997805000825, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 26/32 start...\n",
      "round 26/32 end, effect subset is 0.7945994234526443, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 27/32 start...\n",
      "round 27/32 end, effect subset is 0.7561999004760872, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 28/32 start...\n",
      "round 28/32 end, effect subset is 0.7603985805479961, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 29/32 start...\n",
      "round 29/32 end, effect subset is 0.7843995426604954, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 30/32 start...\n",
      "round 30/32 end, effect subset is 0.7603985805479961, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 31/32 start...\n",
      "round 31/32 end, effect subset is 0.777798701964289, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 32/32 start...\n",
      "round 32/32 end, effect subset is 0.7941979431244603, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "all round end.\n",
      "best effect is 0.8061998644688829, cost time 1, with best param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "best effect is 0.8061998644688829, best param combination is {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}.\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4], \"min_samples_leaf\": [1, 10, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# 调用方法进行超参数的遍历\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, method=\"grid\", k_fold=3, random_state=666)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T02:56:19.795520Z",
     "start_time": "2019-11-07T02:56:19.777311Z"
    }
   },
   "source": [
    "### 1.2 方法2\n",
    "该方法是方法1的升级版，相当于可以传递自定义的参数至`cross_val_score`中，有关`cross_val_score`的API可以参考[链接](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)。\n",
    "\n",
    "下面的使用中定义了`cross_val_score`的`scoring`和`n_jobs`两个参数。\n",
    "> 该方法中参数`X`, `y`和`cv`参数分别对应param_search中的`train_x`, `train_y`和`cv`，不需要再额外定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:20:39.297366Z",
     "start_time": "2019-12-03T09:18:50.465485Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize effect 0.797015727262909, cost time 12, with feat_dim 300, with param {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "round 1/32 start...\n",
      "round 1/32 end, effect subset is 0.7559000252001008, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 2/32 start...\n",
      "round 2/32 end, effect subset is 0.8192379785519142, cost time 2, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 3/32 start...\n",
      "round 3/32 end, effect subset is 0.8667630342521371, cost time 4, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 4/32 start...\n",
      "round 4/32 end, effect subset is 0.8614912291649166, cost time 4, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 5/32 start...\n",
      "round 5/32 end, effect subset is 0.8192379785519142, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 6/32 start...\n",
      "round 6/32 end, effect subset is 0.7559000252001008, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "round 7/32 start...\n",
      "round 7/32 end, effect subset is 0.8741245948983796, cost time 5, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 8/32 start...\n",
      "round 8/32 end, effect subset is 0.8742125008500035, cost time 6, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 9/32 start...\n",
      "round 9/32 end, effect subset is 0.7559000252001008, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 10/32 start...\n",
      "round 10/32 end, effect subset is 0.8522578138312553, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 11/32 start...\n",
      "round 11/32 end, effect subset is 0.8614912291649166, cost time 5, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 12/32 start...\n",
      "round 12/32 end, effect subset is 0.8192379785519142, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 13/32 start...\n",
      "round 13/32 end, effect subset is 0.8495188836755346, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 14/32 start...\n",
      "round 14/32 end, effect subset is 0.874418327273309, cost time 6, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 15/32 start...\n",
      "round 15/32 end, effect subset is 0.7560572258289032, cost time 2, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 16/32 start...\n",
      "round 16/32 end, effect subset is 0.8192379785519142, cost time 3, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 17/32 start...\n",
      "round 17/32 end, effect subset is 0.870741327765311, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 18/32 start...\n",
      "round 18/32 end, effect subset is 0.8605186732746931, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 19/32 start...\n",
      "round 19/32 end, effect subset is 0.8695355309421238, cost time 4, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 20/32 start...\n",
      "round 20/32 end, effect subset is 0.7560572258289032, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "round 21/32 start...\n",
      "round 21/32 end, effect subset is 0.8105712062848252, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 22/32 start...\n",
      "round 22/32 end, effect subset is 0.8111081476325905, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 23/32 start...\n",
      "round 23/32 end, effect subset is 0.8542577146308586, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 24/32 start...\n",
      "round 24/32 end, effect subset is 0.8606089680358722, cost time 4, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 25/32 start...\n",
      "round 25/32 end, effect subset is 0.7560572258289032, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 26/32 start...\n",
      "round 26/32 end, effect subset is 0.7559000252001008, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 27/32 start...\n",
      "round 27/32 end, effect subset is 0.8666060896243584, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 28/32 start...\n",
      "round 28/32 end, effect subset is 0.7560572258289032, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 29/32 start...\n",
      "round 29/32 end, effect subset is 0.8496199720798883, cost time 4, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 30/32 start...\n",
      "round 30/32 end, effect subset is 0.8105712062848252, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 31/32 start...\n",
      "round 31/32 end, effect subset is 0.8542577146308586, cost time 2, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 32/32 start...\n",
      "round 32/32 end, effect subset is 0.8105712062848252, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "all round end.\n",
      "best effect is 0.874418327273309, cost time 6, with best param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "best effect is 0.874418327273309, best param combination is {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}.\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4], \"min_samples_leaf\": [1, 10, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# 定义`cross_val_score`方法的参数\n",
    "cross_val_param = {\"scoring\": lambda clf, X, y: roc_auc_score(y_true=y, y_score=clf.predict_proba(X)[:, 1]),\n",
    "                   \"n_jobs\": None}\n",
    "\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, method=\"grid\", random_state=666, k_fold=5, cross_val_param=cross_val_param)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 方法3\n",
    "\n",
    "上面两种方式的核心是通过交叉验证的方式进行参数组合的评估，显然当数据量较大时速度会很慢。下面这种方式是通过随机划分验证集的方式进行参数组合的评估。   \n",
    "此时需要指定三组参数`create_valid`&`valid_ratio`&`metric_func`，参数`create_valid`是表示需要从输入数据中划分验证集，`valid_ratio`表示划分验证集的比例，最后一组参数`metric_func`则是表示评估验证效果的函数（输入是y_true,y_pred，返回应当只有一个值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:21:02.301398Z",
     "start_time": "2019-12-03T09:20:40.199218Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize effect 0.7970273245921329, cost time 2, with feat_dim 300, with param {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "round 1/32 start...\n",
      "round 1/32 end, effect subset is 0.8370213191872685, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 2/32 start...\n",
      "round 2/32 end, effect subset is 0.8595375838254429, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 3/32 start...\n",
      "round 3/32 end, effect subset is 0.8809008107296568, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 4/32 start...\n",
      "round 4/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 5/32 start...\n",
      "round 5/32 end, effect subset is 0.8734140726653988, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 6/32 start...\n",
      "round 6/32 end, effect subset is 0.8553297968171355, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 7/32 start...\n",
      "round 7/32 end, effect subset is 0.8370213191872685, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 8/32 start...\n",
      "round 8/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 9/32 start...\n",
      "round 9/32 end, effect subset is 0.8804864377940147, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 10/32 start...\n",
      "round 10/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 11/32 start...\n",
      "round 11/32 end, effect subset is 0.8370213191872685, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 12/32 start...\n",
      "round 12/32 end, effect subset is 0.857882093884496, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 13/32 start...\n",
      "round 13/32 end, effect subset is 0.8603062756480833, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 14/32 start...\n",
      "round 14/32 end, effect subset is 0.8370213191872685, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 15/32 start...\n",
      "round 15/32 end, effect subset is 0.8625663096787108, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 16/32 start...\n",
      "round 16/32 end, effect subset is 0.8769732759483535, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 17/32 start...\n",
      "round 17/32 end, effect subset is 0.8735602041837653, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 18/32 start...\n",
      "round 18/32 end, effect subset is 0.888908017215494, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 19/32 start...\n",
      "round 19/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "round 20/32 start...\n",
      "round 20/32 end, effect subset is 0.8124572114903412, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 21/32 start...\n",
      "round 21/32 end, effect subset is 0.8119327394655189, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 22/32 start...\n",
      "round 22/32 end, effect subset is 0.8584025623060755, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 23/32 start...\n",
      "round 23/32 end, effect subset is 0.8595375838254429, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 24/32 start...\n",
      "round 24/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 25/32 start...\n",
      "round 25/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 26/32 start...\n",
      "round 26/32 end, effect subset is 0.879511560404364, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 27/32 start...\n",
      "round 27/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 28/32 start...\n",
      "round 28/32 end, effect subset is 0.8124572114903412, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 29/32 start...\n",
      "round 29/32 end, effect subset is 0.8584025623060755, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 30/32 start...\n",
      "round 30/32 end, effect subset is 0.8124572114903412, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 31/32 start...\n",
      "round 31/32 end, effect subset is 0.8878170353317987, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 32/32 start...\n",
      "round 32/32 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "all round end.\n",
      "best effect is 0.888908017215494, cost time 0, with best param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "best effect is 0.888908017215494, best param combination is {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}.\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4], \"min_samples_leaf\": [1, 10, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, create_valid=True, valid_ratio=0.2, metric_func=roc_auc_score, random_state=666)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 方法4\n",
    "方法3中通过随机划分验证集进行参数的评估，当训练样本中虽在时序相关时（即样本是严格按照时间来产生）时，这种方式欠妥，存在拿未来的样本训练来预测过去的样本。    \n",
    "下面这种方式支持自定义输入验证集。需要指定参数`valid_x`&`valid_y`&`metric_func`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:21:31.114111Z",
     "start_time": "2019-12-03T09:21:02.789525Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize effect 1.0, cost time 3, with feat_dim 300, with param {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "round 1/32 start...\n",
      "round 1/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 2/32 start...\n",
      "round 2/32 end, effect subset is 0.8666801948051948, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 3/32 start...\n",
      "round 3/32 end, effect subset is 0.8851461038961038, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 4/32 start...\n",
      "round 4/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 5/32 start...\n",
      "round 5/32 end, effect subset is 0.8839285714285715, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 6/32 start...\n",
      "round 6/32 end, effect subset is 0.8841314935064934, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 7/32 start...\n",
      "round 7/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 8/32 start...\n",
      "round 8/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 9/32 start...\n",
      "round 9/32 end, effect subset is 0.8851461038961038, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 10/32 start...\n",
      "round 10/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 11/32 start...\n",
      "round 11/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 12/32 start...\n",
      "round 12/32 end, effect subset is 0.8654626623376623, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 13/32 start...\n",
      "round 13/32 end, effect subset is 0.8654626623376623, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 200}\n",
      "round 14/32 start...\n",
      "round 14/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 15/32 start...\n",
      "round 15/32 end, effect subset is 0.8841314935064934, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 16/32 start...\n",
      "round 16/32 end, effect subset is 0.895698051948052, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 17/32 start...\n",
      "round 17/32 end, effect subset is 0.8975243506493507, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "round 18/32 start...\n",
      "round 18/32 end, effect subset is 0.8961038961038961, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 19/32 start...\n",
      "round 19/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "round 20/32 start...\n",
      "round 20/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 10}\n",
      "round 21/32 start...\n",
      "round 21/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 22/32 start...\n",
      "round 22/32 end, effect subset is 0.8666801948051948, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 23/32 start...\n",
      "round 23/32 end, effect subset is 0.8666801948051948, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 24/32 start...\n",
      "round 24/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 25/32 start...\n",
      "round 25/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 26/32 start...\n",
      "round 26/32 end, effect subset is 0.903814935064935, cost time 1, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 100}\n",
      "round 27/32 start...\n",
      "round 27/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 28/32 start...\n",
      "round 28/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 29/32 start...\n",
      "round 29/32 end, effect subset is 0.8666801948051948, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 10}\n",
      "round 30/32 start...\n",
      "round 30/32 end, effect subset is 0.8124999999999999, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 100}\n",
      "round 31/32 start...\n",
      "round 31/32 end, effect subset is 0.8961038961038961, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 32/32 start...\n",
      "round 32/32 end, effect subset is 0.7305194805194806, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 100}\n",
      "all round end.\n",
      "best effect is 1.0, cost time 3, with best param {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "best effect is 1.0, best param combination is {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}.\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4], \"min_samples_leaf\": [1, 10, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, valid_x=df[:100], valid_y=label[:100], metric_func=roc_auc_score, random_state=666)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 方法5\n",
    "当验证方式使用验证集进行参数组合的评估时，该方法提供了一种高阶的使用方式。该方式最初想法是想融入`early_stopping_rounds`至调参的过程中，即每次调参时最优训练轮数是一个动态调整的值。\n",
    "\n",
    "该方式需要额外指定一组参数，该参数类型是dict，里面可包含key值为`model_fit_param`或者`set_eval_set`或者`update_param_func`，\n",
    "- `model_fit_param`  \n",
    "模型训练时，`fit`方法包含的参数，是一个dict\n",
    "- `set_eval_set`   \n",
    "在训练时是否指定验证集合，如果设置为True，会在fit时添加一组参数`eval_set = [(valid_x, valid_y)]`，否则则不添加，往往early_stopping_rounds时需要指定。\n",
    "- `update_param_func`   \n",
    "更新参数的function，输入为model和param，输出也是param，在使用early_stopping_rounds需要返回最佳的训练轮次，则可以启用该函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:23:07.975735Z",
     "start_time": "2019-12-03T09:21:31.634513Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize effect 0.9579461515363827, cost time 7, with feat_dim 300, with param {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 99, 'n_jobs': 1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      "round 1/27 start...\n",
      "round 1/27 end, effect subset is 0.9083274947452707, cost time 2, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 1, 'reg_lambda': 0, 'n_estimators': 99}\n",
      "round 2/27 start...\n",
      "round 2/27 end, effect subset is 0.8904173756380742, cost time 4, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 2, 'reg_lambda': 0, 'n_estimators': 64}\n",
      "round 3/27 start...\n",
      "round 3/27 end, effect subset is 0.9575858272445201, cost time 9, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 3, 'reg_lambda': 10, 'n_estimators': 96}\n",
      "round 4/27 start...\n",
      "round 4/27 end, effect subset is 0.9417195475928335, cost time 5, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 2, 'reg_lambda': 10, 'n_estimators': 99}\n",
      "round 5/27 start...\n",
      "round 5/27 end, effect subset is 0.9242558302472226, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 1, 'reg_lambda': 1, 'n_estimators': 44}\n",
      "round 6/27 start...\n",
      "round 6/27 end, effect subset is 0.8049224301871685, cost time 0, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 1, 'reg_lambda': 0, 'n_estimators': 15}\n",
      "round 7/27 start...\n",
      "round 7/27 end, effect subset is 0.949274346912221, cost time 2, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 3, 'reg_lambda': 10, 'n_estimators': 19}\n",
      "round 8/27 start...\n",
      "round 8/27 end, effect subset is 0.945142628365529, cost time 5, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 2, 'reg_lambda': 1, 'n_estimators': 99}\n",
      "round 9/27 start...\n",
      "round 9/27 end, effect subset is 0.9445681113001702, cost time 2, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 3, 'reg_lambda': 0, 'n_estimators': 15}\n",
      "round 10/27 start...\n",
      "round 10/27 end, effect subset is 0.9447582824542089, cost time 5, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 2, 'reg_lambda': 0, 'n_estimators': 98}\n",
      "round 11/27 start...\n",
      "round 11/27 end, effect subset is 0.925464918426584, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 1, 'reg_lambda': 10, 'n_estimators': 52}\n",
      "round 12/27 start...\n",
      "round 12/27 end, effect subset is 0.9554438995095585, cost time 8, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 3, 'reg_lambda': 0, 'n_estimators': 84}\n",
      "round 13/27 start...\n",
      "round 13/27 end, effect subset is 0.9579461515363827, cost time 7, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 3, 'reg_lambda': 1, 'n_estimators': 99}\n",
      "round 14/27 start...\n",
      "round 14/27 end, effect subset is 0.8721969772795517, cost time 2, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 2, 'reg_lambda': 10, 'n_estimators': 28}\n",
      "round 15/27 start...\n",
      "round 15/27 end, effect subset is 0.8993013712341107, cost time 4, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 3, 'reg_lambda': 1, 'n_estimators': 46}\n",
      "round 16/27 start...\n",
      "round 16/27 end, effect subset is 0.8956620958862977, cost time 2, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 3, 'reg_lambda': 10, 'n_estimators': 28}\n",
      "round 17/27 start...\n",
      "round 17/27 end, effect subset is 0.9070823741367231, cost time 2, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 1, 'reg_lambda': 10, 'n_estimators': 99}\n",
      "round 18/27 start...\n",
      "round 18/27 end, effect subset is 0.9516044439995996, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 3, 'reg_lambda': 1, 'n_estimators': 16}\n",
      "round 19/27 start...\n",
      "round 19/27 end, effect subset is 0.8049224301871685, cost time 0, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 1, 'reg_lambda': 1, 'n_estimators': 15}\n",
      "round 20/27 start...\n",
      "round 20/27 end, effect subset is 0.9426563907516765, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 2, 'reg_lambda': 10, 'n_estimators': 28}\n",
      "round 21/27 start...\n",
      "round 21/27 end, effect subset is 0.9088379541587428, cost time 2, with feature dim is 300, with param {'learning_rate': 0.1, 'max_depth': 1, 'reg_lambda': 1, 'n_estimators': 99}\n",
      "round 22/27 start...\n",
      "round 22/27 end, effect subset is 0.890481433289961, cost time 3, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 2, 'reg_lambda': 1, 'n_estimators': 63}\n",
      "round 23/27 start...\n",
      "round 23/27 end, effect subset is 0.9247082374136724, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 1, 'reg_lambda': 0, 'n_estimators': 42}\n",
      "round 24/27 start...\n",
      "round 24/27 end, effect subset is 0.8996917225502953, cost time 3, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 3, 'reg_lambda': 0, 'n_estimators': 41}\n",
      "round 25/27 start...\n",
      "round 25/27 end, effect subset is 0.9452907616855171, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 2, 'reg_lambda': 1, 'n_estimators': 30}\n",
      "round 26/27 start...\n",
      "round 26/27 end, effect subset is 0.7886457812030828, cost time 0, with feature dim is 300, with param {'learning_rate': 0.01, 'max_depth': 1, 'reg_lambda': 10, 'n_estimators': 2}\n",
      "round 27/27 start...\n",
      "round 27/27 end, effect subset is 0.9392132919627666, cost time 1, with feature dim is 300, with param {'learning_rate': 0.5, 'max_depth': 2, 'reg_lambda': 0, 'n_estimators': 24}\n",
      "all round end.\n",
      "best effect is 0.9579461515363827, cost time 7, with best param {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 99, 'n_jobs': 1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      "best effect is 0.9579461515363827, best param combination is {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 99, 'n_jobs': 1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}.\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = XGBClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3], \"learning_rate\": [0.01, 0.5, 0.1], \"reg_lambda\": [0, 1, 10]}\n",
    "\n",
    "def _update(model, param):\n",
    "    if param is None:\n",
    "        return model.get_params()\n",
    "    else:\n",
    "        param[\"n_estimators\"] = model.best_iteration\n",
    "    return param\n",
    "valid_set_param = {\"model_fit_param\": {\"eval_metric\": \"auc\", \"verbose\": False, \"early_stopping_rounds\": 5},\n",
    "                   \"set_eval_set\": True,\n",
    "                   \"update_param_func\": _update}\n",
    "\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, method=\"grid\", random_state=666, create_valid=True,\n",
    "                                       valid_ratio=0.2, metric_func=roc_auc_score, valid_set_param=valid_set_param)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T05:53:07.397810Z",
     "start_time": "2019-11-07T05:53:07.391057Z"
    }
   },
   "source": [
    "### 1.6 方法6\n",
    "以上方式均是单机版一次次进行遍历进行，如果内存允许的情况下，可以启用多进程。下面以交叉验证作为参数组合评选标准进行，对验证集的方式评估该方式完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:26:08.409554Z",
     "start_time": "2019-12-03T09:23:08.509431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best effect is 0.7974031184031184, best param combination is {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}.\n",
      "do not use multiprocess cost time 101\n",
      "best effect is 0.7974031184031184, best param combination is {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 1}.\n",
      "use multiprocess cost time 78\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4], \"min_samples_leaf\": [1, 10, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "# 未启用多进程，测试耗时\n",
    "s = datetime.now()\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, method=\"grid\", k_fold=5, random_state=666, verbose=False)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")\n",
    "e = datetime.now()\n",
    "print(f\"do not use multiprocess cost time {(e-s).seconds}\")  \n",
    "    \n",
    "# 启用多进程，测试耗时\n",
    "s = datetime.now()\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, method=\"grid\", k_fold=5, random_state=666,\n",
    "             enable_multiprocess=True, n_jobs=2, verbose=False)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")\n",
    "e = datetime.now()\n",
    "print(f\"use multiprocess cost time {(e-s).seconds}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:05:11.683235Z",
     "start_time": "2019-11-07T06:05:11.669173Z"
    }
   },
   "source": [
    "### 1.7 方法7\n",
    "该方式可以指定算法为随机搜索，以及对应的随机搜索的轮次，其余使用方式和方面的使用方式完全一致，下面随便挑选一种方式示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:26:18.860221Z",
     "start_time": "2019-12-03T09:26:09.177419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize effect 0.7970273245921329, cost time 2, with feat_dim 300, with param {'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "round 1/10 start...\n",
      "round 1/10 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 200}\n",
      "round 2/10 start...\n",
      "round 2/10 end, effect subset is 0.8370213191872685, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 1}\n",
      "round 3/10 start...\n",
      "round 3/10 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 1, 'min_samples_leaf': 10}\n",
      "round 4/10 start...\n",
      "round 4/10 end, effect subset is 0.7768591732559305, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 1}\n",
      "round 5/10 start...\n",
      "round 5/10 end, effect subset is 0.8370213191872685, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 2, 'min_samples_leaf': 200}\n",
      "round 6/10 start...\n",
      "round 6/10 end, effect subset is 0.8595375838254429, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "round 7/10 start...\n",
      "round 7/10 end, effect subset is 0.8763847462716445, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 1}\n",
      "round 8/10 start...\n",
      "round 8/10 end, effect subset is 0.8809008107296568, cost time 1, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "round 9/10 start...\n",
      "round 9/10 end, effect subset is 0.8553297968171355, cost time 0, with feature dim is 300, with param {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 100}\n",
      "round 10/10 start...\n",
      "round 10/10 end, effect subset is 0.8734140726653988, cost time 0, with feature dim is 300, with param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 200}\n",
      "all round end.\n",
      "best effect is 0.8809008107296568, cost time 1, with best param {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}\n",
      "best effect is 0.8809008107296568, best param combination is {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10}.\n"
     ]
    }
   ],
   "source": [
    "# 定义使用的模型和所需要的参数网格\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4], \"min_samples_leaf\": [1, 10, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "best_effect, best_param = param_search(df, label, clf, param_grid, method=\"random\", max_iter=10,\n",
    "                                       create_valid=True, valid_ratio=0.2, \n",
    "                                       metric_func=roc_auc_score, random_state=666)\n",
    "print(f\"best effect is {best_effect}, best param combination is {best_param}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:50.455067Z",
     "start_time": "2019-11-07T06:34:50.443663Z"
    }
   },
   "source": [
    "### 1.8 分布式调参\n",
    "该方式和上面的介绍一致，但融入了spark的分布式计算能力，首先在driver端读取数据，之后分发数据至executor端，然后进行参数组合的遍历，\n",
    "之后汇聚每组参数的效果至driver端，进行比较选择最优的参数。\n",
    "\n",
    "其使用方式和上面基本一致，不同之处需要传入一个spark session，因为本地没有搭建分布式环境，具体的使用方式可以参考目录`../tests/test_distribute_param_search.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. bayes_opt\n",
    "\n",
    "该方式主要对bayes_opt包进行了封装，该包的具体使用可以参考[链接](https://github.com/rmcantin/bayesopt)。\n",
    "封装主要保留了可以指定交叉验证或者验证集评估的方式，其余类似更新参数和多进程方式目前均不支持。   \n",
    "\n",
    "### 2.1 方法1\n",
    "该方式通过交叉验证的来评估每一轮的迭代效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:17:52.922355Z",
     "start_time": "2019-11-07T08:17:02.020954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7212  \u001b[0m | \u001b[0m 0.6603  \u001b[0m | \u001b[0m 21.42   \u001b[0m | \u001b[0m 172.4   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6991  \u001b[0m | \u001b[0m 0.6823  \u001b[0m | \u001b[0m 23.88   \u001b[0m | \u001b[0m 13.05   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5925  \u001b[0m | \u001b[0m 0.4309  \u001b[0m | \u001b[0m 3.123   \u001b[0m | \u001b[0m 33.98   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.731   \u001b[0m | \u001b[95m 0.5065  \u001b[0m | \u001b[95m 6.606   \u001b[0m | \u001b[95m 188.6   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6801  \u001b[0m | \u001b[0m 0.2543  \u001b[0m | \u001b[0m 18.12   \u001b[0m | \u001b[0m 80.37   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7089  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 250.0   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7004  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 250.0   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7102  \u001b[0m | \u001b[0m 0.1863  \u001b[0m | \u001b[0m 24.99   \u001b[0m | \u001b[0m 211.2   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6593  \u001b[0m | \u001b[0m 0.1524  \u001b[0m | \u001b[0m 2.043   \u001b[0m | \u001b[0m 142.3   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6912  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 47.53   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6599  \u001b[0m | \u001b[0m 0.7396  \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 113.7   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7096  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 2.206   \u001b[0m | \u001b[0m 211.1   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.7414  \u001b[0m | \u001b[95m 0.8953  \u001b[0m | \u001b[95m 2.053   \u001b[0m | \u001b[95m 181.4   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6906  \u001b[0m | \u001b[0m 0.141   \u001b[0m | \u001b[0m 2.011   \u001b[0m | \u001b[0m 175.9   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7108  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 100.8   \u001b[0m |\n",
      "=============================================================\n",
      "best_result is 0.7414215686274509, best_param is {'max_features': 0.8952531398122066, 'min_samples_split': 2, 'n_estimators': 181}\n"
     ]
    }
   ],
   "source": [
    "from model_helper.hyper_parameter_tuning import bayes_search\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\"max_features\": {\"interval\": (0.1, 0.9), \"type\": float},\n",
    "               \"n_estimators\": {\"interval\": (10, 250), \"type\": int},\n",
    "               \"min_samples_split\": {\"interval\": (2, 25), \"type\": int}\n",
    "               }\n",
    "best_result, best_params = bayes_search(df[:100], label[:100], model=clf, param_space=param_space, n_iter=10,\n",
    "                                        k_fold=3, random_state=666)\n",
    "print(f\"best_result is {best_result}, best_param is {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T07:34:17.725268Z",
     "start_time": "2019-11-07T07:34:17.698907Z"
    }
   },
   "source": [
    "### 2.2 方法2\n",
    "通过随机产生验证集的方式进行效果的评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:18:44.272244Z",
     "start_time": "2019-11-07T08:17:52.925137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8125  \u001b[0m | \u001b[0m 0.6603  \u001b[0m | \u001b[0m 21.42   \u001b[0m | \u001b[0m 172.4   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7917  \u001b[0m | \u001b[0m 0.6823  \u001b[0m | \u001b[0m 23.88   \u001b[0m | \u001b[0m 13.05   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.651   \u001b[0m | \u001b[0m 0.4309  \u001b[0m | \u001b[0m 3.123   \u001b[0m | \u001b[0m 33.98   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7917  \u001b[0m | \u001b[0m 0.5065  \u001b[0m | \u001b[0m 6.606   \u001b[0m | \u001b[0m 188.6   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7917  \u001b[0m | \u001b[0m 0.2543  \u001b[0m | \u001b[0m 18.12   \u001b[0m | \u001b[0m 80.37   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7708  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 250.0   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.875   \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 25.0    \u001b[0m | \u001b[95m 123.4   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7917  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 250.0   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8646  \u001b[0m | \u001b[0m 0.8053  \u001b[0m | \u001b[0m 2.001   \u001b[0m | \u001b[0m 137.0   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8021  \u001b[0m | \u001b[0m 0.1183  \u001b[0m | \u001b[0m 24.81   \u001b[0m | \u001b[0m 134.6   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8125  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 104.0   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8021  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 214.3   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8594  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.75    \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 101.0   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7188  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 54.34   \u001b[0m |\n",
      "=============================================================\n",
      "best_result is 0.875, best_param is {'max_features': 0.9, 'min_samples_split': 25, 'n_estimators': 123}\n"
     ]
    }
   ],
   "source": [
    "from model_helper.hyper_parameter_tuning import bayes_search\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\"max_features\": {\"interval\": (0.1, 0.9), \"type\": float},\n",
    "               \"n_estimators\": {\"interval\": (10, 250), \"type\": int},\n",
    "               \"min_samples_split\": {\"interval\": (2, 25), \"type\": int}\n",
    "               }\n",
    "best_result, best_params = bayes_search(df[:100], label[:100], model=clf, param_space=param_space, n_iter=10,\n",
    "                                        create_valid=True, valid_ratio=0.2, metric_func=roc_auc_score, random_state=666)\n",
    "print(f\"best_result is {best_result}, best_param is {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 方法3\n",
    "通过自定义验证集的方式进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:19:35.386021Z",
     "start_time": "2019-11-07T08:18:44.274953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_fe... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8117  \u001b[0m | \u001b[0m 0.6603  \u001b[0m | \u001b[0m 21.42   \u001b[0m | \u001b[0m 172.4   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.7597  \u001b[0m | \u001b[0m 0.6823  \u001b[0m | \u001b[0m 23.88   \u001b[0m | \u001b[0m 13.05   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 0.4309  \u001b[0m | \u001b[0m 3.123   \u001b[0m | \u001b[0m 33.98   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7955  \u001b[0m | \u001b[0m 0.5065  \u001b[0m | \u001b[0m 6.606   \u001b[0m | \u001b[0m 188.6   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 0.2543  \u001b[0m | \u001b[0m 18.12   \u001b[0m | \u001b[0m 80.37   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7841  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 25.0    \u001b[0m | \u001b[0m 250.0   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8084  \u001b[0m | \u001b[0m 0.1417  \u001b[0m | \u001b[0m 24.98   \u001b[0m | \u001b[0m 197.5   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7987  \u001b[0m | \u001b[0m 0.1091  \u001b[0m | \u001b[0m 24.83   \u001b[0m | \u001b[0m 157.2   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 0.8963  \u001b[0m | \u001b[0m 2.087   \u001b[0m | \u001b[0m 123.2   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8117  \u001b[0m | \u001b[0m 0.8722  \u001b[0m | \u001b[0m 24.99   \u001b[0m | \u001b[0m 185.1   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7833  \u001b[0m | \u001b[0m 0.878   \u001b[0m | \u001b[0m 2.343   \u001b[0m | \u001b[0m 157.1   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7914  \u001b[0m | \u001b[0m 0.4115  \u001b[0m | \u001b[0m 2.225   \u001b[0m | \u001b[0m 249.9   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7784  \u001b[0m | \u001b[0m 0.8303  \u001b[0m | \u001b[0m 2.171   \u001b[0m | \u001b[0m 10.08   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7354  \u001b[0m | \u001b[0m 0.1258  \u001b[0m | \u001b[0m 2.002   \u001b[0m | \u001b[0m 87.69   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7987  \u001b[0m | \u001b[0m 0.8889  \u001b[0m | \u001b[0m 24.85   \u001b[0m | \u001b[0m 115.4   \u001b[0m |\n",
      "=============================================================\n",
      "best_result is 0.8116883116883117, best_param is {'max_features': 0.6603496974862677, 'min_samples_split': 21, 'n_estimators': 172}\n"
     ]
    }
   ],
   "source": [
    "from model_helper.hyper_parameter_tuning import bayes_search\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\"max_features\": {\"interval\": (0.1, 0.9), \"type\": float},\n",
    "               \"n_estimators\": {\"interval\": (10, 250), \"type\": int},\n",
    "               \"min_samples_split\": {\"interval\": (2, 25), \"type\": int}\n",
    "               }\n",
    "best_result, best_params = bayes_search(df[:100], label[:100], model=clf, param_space=param_space, n_iter=10,\n",
    "                                        valid_x=df[100:150], valid_y=label[100:150], metric_func=roc_auc_score, random_state=666)\n",
    "print(f\"best_result is {best_result}, best_param is {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. hyper_opt\n",
    "\n",
    "该方式主要对hyper_opt包进行了封装，该包的具体使用可以参考[链接](http://hyperopt.github.io/hyperopt/)。\n",
    "封装主要保留了可以指定交叉验证或者验证集评估的方式，其余类似更新参数和多进程方式目前均不支持。   \n",
    "\n",
    "### 2.1 方法1\n",
    "该方式通过交叉验证的来评估每一轮的迭代效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:19:38.594406Z",
     "start_time": "2019-11-07T08:19:35.387836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.32it/s, best loss: -0.701593137254902]\n",
      "{'param': {'max_features': 0.7717534098692105, 'min_samples_split': 10, 'n_estimators': 63}, 'effect': 0.6415441176470589}\n",
      "{'param': {'max_features': 0.26018003099419995, 'min_samples_split': 10, 'n_estimators': 61}, 'effect': 0.7009803921568628}\n",
      "{'param': {'max_features': 0.21371575812946564, 'min_samples_split': 2, 'n_estimators': 41}, 'effect': 0.6709558823529411}\n",
      "{'param': {'max_features': 0.41707981574580155, 'min_samples_split': 10, 'n_estimators': 19}, 'effect': 0.6819852941176471}\n",
      "{'param': {'max_features': 0.6356237134216275, 'min_samples_split': 100, 'n_estimators': 69}, 'effect': 0.5600490196078431}\n",
      "{'param': {'max_features': 0.6379048296438755, 'min_samples_split': 10, 'n_estimators': 19}, 'effect': 0.6617647058823529}\n",
      "{'param': {'max_features': 0.4336835533075999, 'min_samples_split': 100, 'n_estimators': 78}, 'effect': 0.5600490196078431}\n",
      "{'param': {'max_features': 0.8945511738511079, 'min_samples_split': 2, 'n_estimators': 83}, 'effect': 0.701593137254902}\n",
      "{'param': {'max_features': 0.6755383512314407, 'min_samples_split': 100, 'n_estimators': 80}, 'effect': 0.5600490196078431}\n",
      "{'param': {'max_features': 0.7129091426747338, 'min_samples_split': 10, 'n_estimators': 67}, 'effect': 0.6721813725490197}\n",
      "best_param is {'max_features': 0.8945511738511079, 'min_samples_split': 0, 'n_estimators': 73}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from model_helper.hyper_parameter_tuning import hyperopt_search\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\"max_features\": hp.uniform(\"max_features\", 0.1, 0.9),\n",
    "               \"n_estimators\": hp.choice(\"n_estimators\", range(10, 100)),\n",
    "               \"min_samples_split\": hp.choice(\"min_samples_split\", [2, 10, 100])\n",
    "               }\n",
    "trials, best_params = hyperopt_search(df[:100], label[:100], model=clf, param_space=param_space, n_iter=10,\n",
    "                                      k_fold=3, random_state=666)\n",
    "for i in trials:\n",
    "    print(i[\"result\"][\"stuff\"])\n",
    "print(f\"best_param is {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 方法2\n",
    "通过随机产生验证集的方式进行效果的评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:19:39.920957Z",
     "start_time": "2019-11-07T08:19:38.597016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.15it/s, best loss: -0.9583333333333334]\n",
      "{'param': {'max_features': 0.7717534098692105, 'min_samples_split': 10, 'n_estimators': 63}, 'effect': 0.78125}\n",
      "{'param': {'max_features': 0.26018003099419995, 'min_samples_split': 10, 'n_estimators': 61}, 'effect': 0.7291666666666667}\n",
      "{'param': {'max_features': 0.21371575812946564, 'min_samples_split': 2, 'n_estimators': 41}, 'effect': 0.765625}\n",
      "{'param': {'max_features': 0.41707981574580155, 'min_samples_split': 10, 'n_estimators': 19}, 'effect': 0.9583333333333334}\n",
      "{'param': {'max_features': 0.6356237134216275, 'min_samples_split': 100, 'n_estimators': 69}, 'effect': 0.5}\n",
      "{'param': {'max_features': 0.6379048296438755, 'min_samples_split': 10, 'n_estimators': 19}, 'effect': 0.65625}\n",
      "{'param': {'max_features': 0.4336835533075999, 'min_samples_split': 100, 'n_estimators': 78}, 'effect': 0.5}\n",
      "{'param': {'max_features': 0.8945511738511079, 'min_samples_split': 2, 'n_estimators': 83}, 'effect': 0.8177083333333334}\n",
      "{'param': {'max_features': 0.6755383512314407, 'min_samples_split': 100, 'n_estimators': 80}, 'effect': 0.5}\n",
      "{'param': {'max_features': 0.7129091426747338, 'min_samples_split': 10, 'n_estimators': 67}, 'effect': 0.8125}\n",
      "best_param is {'max_features': 0.41707981574580155, 'min_samples_split': 1, 'n_estimators': 9}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from model_helper.hyper_parameter_tuning import hyperopt_search\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\"max_features\": hp.uniform(\"max_features\", 0.1, 0.9),\n",
    "               \"n_estimators\": hp.choice(\"n_estimators\", range(10, 100)),\n",
    "               \"min_samples_split\": hp.choice(\"min_samples_split\", [2, 10, 100])\n",
    "               }\n",
    "trials, best_params = hyperopt_search(df[:100], label[:100], model=clf, param_space=param_space, n_iter=10,\n",
    "                                      create_valid=True, valid_ratio=0.2, metric_func=roc_auc_score, random_state=666)\n",
    "for i in trials:\n",
    "    print(i[\"result\"][\"stuff\"])\n",
    "print(f\"best_param is {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 方法3\n",
    "通过自定义验证集的方式进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T08:19:41.883336Z",
     "start_time": "2019-11-07T08:19:39.923320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.21it/s, best loss: -0.814935064935065]\n",
      "{'param': {'max_features': 0.7717534098692105, 'min_samples_split': 10, 'n_estimators': 63}, 'effect': 0.7775974025974026}\n",
      "{'param': {'max_features': 0.26018003099419995, 'min_samples_split': 10, 'n_estimators': 61}, 'effect': 0.7483766233766234}\n",
      "{'param': {'max_features': 0.21371575812946564, 'min_samples_split': 2, 'n_estimators': 41}, 'effect': 0.7402597402597403}\n",
      "{'param': {'max_features': 0.41707981574580155, 'min_samples_split': 10, 'n_estimators': 19}, 'effect': 0.635551948051948}\n",
      "{'param': {'max_features': 0.6356237134216275, 'min_samples_split': 100, 'n_estimators': 69}, 'effect': 0.5}\n",
      "{'param': {'max_features': 0.6379048296438755, 'min_samples_split': 10, 'n_estimators': 19}, 'effect': 0.814935064935065}\n",
      "{'param': {'max_features': 0.4336835533075999, 'min_samples_split': 100, 'n_estimators': 78}, 'effect': 0.5}\n",
      "{'param': {'max_features': 0.8945511738511079, 'min_samples_split': 2, 'n_estimators': 83}, 'effect': 0.7938311688311688}\n",
      "{'param': {'max_features': 0.6755383512314407, 'min_samples_split': 100, 'n_estimators': 80}, 'effect': 0.5}\n",
      "{'param': {'max_features': 0.7129091426747338, 'min_samples_split': 10, 'n_estimators': 67}, 'effect': 0.7873376623376623}\n",
      "best_param is {'max_features': 0.6379048296438755, 'min_samples_split': 1, 'n_estimators': 9}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from model_helper.hyper_parameter_tuning import hyperopt_search\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_space = {\"max_features\": hp.uniform(\"max_features\", 0.1, 0.9),\n",
    "               \"n_estimators\": hp.choice(\"n_estimators\", range(10, 100)),\n",
    "               \"min_samples_split\": hp.choice(\"min_samples_split\", [2, 10, 100])\n",
    "               }\n",
    "trials, best_params = hyperopt_search(df[:100], label[:100], model=clf, param_space=param_space, n_iter=10,\n",
    "                                      valid_x=df[100:150], valid_y=label[100:150], metric_func=roc_auc_score, random_state=666)\n",
    "for i in trials:\n",
    "    print(i[\"result\"][\"stuff\"])\n",
    "print(f\"best_param is {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
